import jsonlines
import argparse
import logging
import json
import os
import sys
import hashlib
import re
import collections
import dataclasses
from typing import Dict, List, Optional, Tuple, Any

import sglang as sgl
from sglang.srt.server_args import ServerArgs
from tqdm import tqdm
from transformers import AutoTokenizer

logging.basicConfig(level=logging.INFO)


# ==================== Format Validation Functions ====================

def parse_generation(content: str) -> tuple[str, list[dict], str]:
    """
    Extract generation from query

    Args:
        content: Text generated by model

    Returns:
        (think, tool_call_objs, reply)
    """
    if "<think>" not in content:
        content = "<think>" + content
    think = re.findall(r"<think>(.*?)</think>", content, re.S)
    if len(think) != 1:
        raise ValueError(f"number of <think></think> tags is not 1")
    think = think[0].strip()

    answer = content.split("</think>")[-1].strip()

    tool_call_objs = []
    reply = None
    if "<tool_call>" in answer:
        tool_calls = re.findall(r"<tool_call>(.*?)</tool_call>", answer, re.S)
        for tool_call in tool_calls:
            tool_call_obj = json.loads(tool_call)
            name = tool_call_obj["name"]
            args = tool_call_obj["arguments"]
            tool_call_objs.append({"name": name, "arguments": args})
    else:
        reply = answer

    return think, tool_call_objs, reply


def extract_tools_from_prompt(prompt: str) -> List[str]:
    """
    Extract tools from prompt

    Args:
        prompt: Prompt text containing <tools>...</tools>

    Returns:
        Tool dictionary {name: parameters}
    """
    tools = re.findall(r"<tools>(.*?)</tools>", prompt, re.S)
    if len(tools) == 0:
        return []
    else:
        tools = tools[-1]

    tool_dict = {}

    for tool in tools.split("\n"):
        tool = tool.strip()
        if tool:
            tool_obj = json.loads(tool)
            # Compatible with OpenAI format
            if "type" in tool_obj and "function" in tool_obj:
                tool_obj = tool_obj['function']
            name = tool_obj["name"]
            parameters = tool_obj["parameters"]
            # Compatible with OpenAI format
            if 'properties' in parameters and isinstance(parameters['properties'], dict) and parameters.get("type") is not None:
                parameters = parameters['properties']
            tool_dict[name] = parameters

    return tool_dict


def check_tool_calls_valid(tool_calls: List[Dict[str, Any]], tool_dict: Dict[str, Any]) -> bool:
    """
    Check if tool calls are valid

    Args:
        tool_calls: Tool calls generated by model
        tool_dict: Tool dictionary extracted from prompt

    Returns:
        Whether valid
    """
    for tool_call in tool_calls:
        name = tool_call["name"]
        if name not in tool_dict:
            return False, "name not in tool_dict"

        args = tool_call["arguments"]
        parameters = tool_dict[name]

        for param in args:
            if param not in parameters:
                return False, "args not in parameters"

    return True, 'everything is good'


def check_format_valid(generation, prompt_text):
    """
    Check if the generated content format is valid

    Args:
        generation: Generated text
        prompt_text: Prompt text

    Returns:
        (is_valid, error_reason)
    """
    try:
        # 1. Parse generation
        p_think, p_tool_call_objs, p_reply = parse_generation(generation)
    except Exception as e:
        return False, f"parse_generation failed: {e}"

    try:
        # 2. Extract tools from prompt
        tool_dict = extract_tools_from_prompt(prompt_text)
    except Exception as e:
        return False, f"extract_tools_from_prompt failed: {e}"

    try:
        # 3. Check if tool_calls are valid
        if p_tool_call_objs:
            result, reason = check_tool_calls_valid(p_tool_call_objs, tool_dict)
            if not result:
                return False, f"tool_calls validation failed: {reason}"
    except Exception as e:
        return False, f"tool_calls validation error: {e}"

    return True, "ok"


# ==================== Original Functions ====================

def get_args():
    import argparse
    parser = argparse.ArgumentParser(description="Rollout all data using teacher model and replace messages[-1]")
    parser.add_argument("--input", type=str, required=True, help="Input file path (jsonl format)")
    parser.add_argument("--output", type=str, required=True, help="Output file path")
    parser.add_argument("--temperature", type=float, default=1.0, help="Sampling temperature")
    parser.add_argument("--top-p", type=float, default=1.0, help="Top-p sampling")
    parser.add_argument("--top-k", type=int, default=-1, help="Top-k sampling")
    parser.add_argument("--max-tokens", type=int, default=2048, help="Maximum number of generated tokens")
    parser.add_argument("--batch-size", type=int, default=128, help="Batch size")
    parser.add_argument("--max-rollout", type=int, default=3, help="Maximum rollout attempts")
    parser.add_argument("--save-interval", type=int, default=100, help="Save interval (save every N items)")
    parser.add_argument("--rollout-n", type=int, default=1, help="Number of rollouts per sample")
    parser.add_argument("--remove-existing-file", action="store_true", help="Remove existing output file")
    # Add sglang server arguments
    ServerArgs.add_cli_args(parser)
    args = parser.parse_args()
    args.log_level = "warning"
    return args


def read_conversations(input_path: str) -> List[Dict]:
    """Read conversation data"""
    conversations = []
    with jsonlines.open(input_path) as reader:
        for item in reader:
            conversations.append(item)
    print(f"Read {len(conversations)} conversations")
    return conversations


def messages_to_prompt_text(messages: List[Dict], tokenizer, tools: Optional[List]) -> str:
    """Convert messages list to prompt_text format"""
    prompt_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        tools=tools
    )
    return prompt_text


def parse_assistant_message_from_generation(generation: str) -> Dict:
    """
    Parse assistant message from generated text

    Args:
        generation: Text generated by model

    Returns:
        {"reasoning_content": str, "content": str or None, "tool_calls": list or None}
    """
    try:
        think, tool_call_objs, reply = parse_generation(generation)

        result = {
            "reasoning_content": think if think else None,
            "content": None,
            "tool_calls": None
        }

        if tool_call_objs:
            # Has tool calls
            result["tool_calls"] = tool_call_objs
        elif reply:
            # Pure text reply
            result["content"] = reply

        return result
    except Exception as e:
        logging.warning(f"Failed to parse generated content: {e}")
        return {
            "reasoning_content": None,
            "content": generation if generation else None,
            "tool_calls": None
        }


def message_to_hash(rolled_message: Dict) -> str:
    """
    Convert assistant message to hash for deduplication

    Args:
        rolled_message: Assistant message

    Returns:
        Unique identifier string
    """
    # Create a deterministic representation
    content = rolled_message.get("content")
    tool_calls = rolled_message.get("tool_calls")
    reasoning_content = rolled_message.get("reasoning_content")

    # Normalize to JSON string
    normalized = json.dumps({
        "content": content,
        "tool_calls": sorted(tool_calls, key=lambda x: json.dumps(x, sort_keys=True)) if tool_calls else None,
        "reasoning_content": reasoning_content
    }, sort_keys=True)

    # Calculate hash
    return hashlib.md5(normalized.encode()).hexdigest()


def process_conversations_batch(
    conversations: List[Dict],
    llm: sgl.Engine,
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    max_rollout: int,
    batch_size: int,
    tokenizer,
    output_path: str,
    save_interval: int,
    rollout_n: int = 1
) -> Tuple[List[Dict], int, int, int]:
    """
    Batch process conversations, use batch processing for efficiency, and incrementally save results

    Args:
        conversations: List of conversations
        llm: sglang inference engine
        temperature: Temperature
        top_p: Top-p sampling
        top_k: Top-k sampling
        max_tokens: Maximum number of tokens
        max_rollout: Maximum rollout attempts
        batch_size: Batch size
        tokenizer: Tokenizer instance
        output_path: Output file path
        save_interval: Save interval
        rollout_n: Number of generations per rollout

    Returns:
        (processed_conversations, successful_input_count, discard_count, total_output_count)
    """
    all_results = []
    batch_buffer = []
    success_input_count = 0  # Number of successfully processed input samples
    discard_count = 0
    total_output_count = 0  # Total number of output samples

    # First filter out conversations that need processing (last message is from assistant)
    conversations_to_process = []
    conversations_to_skip = []

    for conv in conversations:
        messages = conv.get("messages", [])
        if not messages or messages[-1].get("role") != "assistant":
            conversations_to_skip.append(conv)
        else:
            conversations_to_process.append(conv)

    print(f"Conversations to process: {len(conversations_to_process)}, Skipped conversations: {len(conversations_to_skip)}")

    # Add directly skipped conversations to results
    for conv in conversations_to_skip:
        all_results.append(conv)
        batch_buffer.append(conv)
        total_output_count += 1
        success_input_count += 1

    # Batch process conversations that need rollout
    for i in tqdm(range(0, len(conversations_to_process), batch_size), desc="Teacher Rollout"):
        batch_conversations = conversations_to_process[i: i + batch_size]

        # Prepare data for batch inference
        prompt_batch = []
        conv_indices = []

        for idx, conv in enumerate(batch_conversations):
            messages = conv.get("messages", [])
            tools = conv.get("tools", [])
            context_messages = messages[:-1]
            prompt_text = messages_to_prompt_text(context_messages, tokenizer, tools)
            # Repeat each prompt rollout_n times
            prompt_batch.append(prompt_text)
            conv_indices.append(idx)

        # Batch inference (including rollout_n repetitions)
        sampling_params = {
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "max_new_tokens": max_tokens,
            "n": rollout_n
        }

        # Try multiple rollout attempts
        for attempt in range(max_rollout):
            try:
                outputs = llm.generate(prompt_batch, sampling_params)

                # Group results by conversation
                conv_results = {}  # {conv_idx: [parsed_messages]}

                conv_indices_full = []
                for idx in conv_indices:
                    conv_indices_full += [idx] * rollout_n

                prompt_batch_full = []
                for idx in conv_indices:
                    prompt_batch_full += [prompt_batch[idx]] * rollout_n
                    
                for prompt_text, output, conv_idx in zip(prompt_batch_full, outputs, conv_indices_full):
                    generation = output['text']

                    is_valid, reason = check_format_valid(generation, prompt_text)
                    if is_valid:
                        parsed_message = parse_assistant_message_from_generation(generation)
                        if conv_idx not in conv_results:
                            conv_results[conv_idx] = []
                        conv_results[conv_idx].append(parsed_message)

                # Process successful conversations
                for conv_idx, parsed_messages in conv_results.items():
                    conv = batch_conversations[conv_idx]
                    messages = conv.get("messages", [])

                    # Deduplicate
                    seen_hashes = set()
                    unique_messages = []
                    for msg in parsed_messages:
                        msg_hash = message_to_hash(msg)
                        if msg_hash not in seen_hashes:
                            seen_hashes.add(msg_hash)
                            unique_messages.append(msg)

                    # Create a conversation for each unique rollout result
                    for rolled_message in unique_messages:
                        new_message = {
                            "role": "assistant",
                            "content": rolled_message.get("content"),
                            "tool_calls": rolled_message.get("tool_calls"),
                            "reasoning_content": rolled_message.get("reasoning_content")
                        }

                        new_messages = messages[:-1] + [new_message]
                        result = conv.copy()
                        result["messages"] = new_messages

                        all_results.append(result)
                        batch_buffer.append(result)
                        total_output_count += 1

                    success_input_count += 1

                # Record failed conversations
                for idx, conv in enumerate(batch_conversations):
                    if idx not in conv_results:
                        discard_count += 1

                # Successfully processed this batch, exit retry loop
                break

            except Exception as e:
                logging.warning(f"Batch inference failed (attempt {attempt+1}/{max_rollout}): {e}")
                if attempt == max_rollout - 1:
                    # Last attempt also failed, discard entire batch
                    discard_count += len(batch_conversations)

        # Save every save_interval items
        if len(batch_buffer) >= save_interval:
            with jsonlines.open(output_path, mode="a") as writer:
                for item in batch_buffer:
                    writer.write(item)
            print(f"Saved {total_output_count} results (inputs: {success_input_count}/{len(conversations)}, discarded: {discard_count})")
            batch_buffer.clear()

    # Save remaining results
    if batch_buffer:
        with jsonlines.open(output_path, mode="a") as writer:
            for item in batch_buffer:
                writer.write(item)
        print(f"Saved {total_output_count} results (inputs: {success_input_count}/{len(conversations)}, discarded: {discard_count})")

    return all_results, success_input_count, discard_count, total_output_count


def main():
    """Main function"""
    args = get_args()
    server_args = ServerArgs.from_cli_args(args)

    print("=" * 80)
    print("Starting Teacher Rollout")
    print(f"Input file: {args.input}")
    print(f"Output file: {args.output}")
    print(f"Model path: {server_args.model_path}")
    print(f"Batch size: {args.batch_size}")
    print(f"Maximum rollout attempts: {args.max_rollout}")
    print(f"Rollouts per sample: {args.rollout_n}")
    print("=" * 80)

    # Clear output file if it exists
    if os.path.exists(args.output):

        if args.remove_existing_file:
            os.remove(args.output)
            print(f"Cleared existing output file: {args.output}")
        else:
            raise Exception(f"Output file already exists: {args.output}")

    # Read conversation data
    conversations = read_conversations(args.input)

    # Load tokenizer
    print(f"Loading tokenizer: {args.tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)

    # Create sglang inference engine
    print("Loading sglang inference engine...")
    llm = sgl.Engine(**dataclasses.asdict(server_args))

    # Batch process conversations
    results, success_input_count, discard_count, total_output_count = process_conversations_batch(
        conversations,
        llm,
        args.temperature,
        args.top_p,
        args.top_k,
        args.max_tokens,
        args.max_rollout,
        args.batch_size,
        tokenizer,
        args.output,
        args.save_interval,
        args.rollout_n
    )

    print("=" * 80)
    print("Processing completed")
    print(f"Total input conversations: {len(conversations)}")
    print(f"Successfully processed inputs: {success_input_count}")
    print(f"Discarded inputs: {discard_count}")
    print(f"Total output samples: {total_output_count}")
    print(f"Average rollout count: {total_output_count/success_input_count if success_input_count > 0 else 0:.2f}")
    print(f"Input success rate: {success_input_count/len(conversations)*100:.2f}%")
    print(f"Results saved to: {args.output}")
    print("=" * 80)


if __name__ == "__main__":
    main()
